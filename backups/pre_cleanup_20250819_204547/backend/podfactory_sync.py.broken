import os
#!/usr/bin/env python3
"""
PodFactory Sync - Updated to store times in UTC
Key changes:
1. All times stored in UTC in database
2. Proper timezone handling for queries
3. Only convert to CT for display
"""

import pymysql
import mysql.connector
import requests
import json
from datetime import datetime, timedelta
import pytz
import time
import logging

# Map PodFactory actions to departments
ACTION_TO_DEPARTMENT_MAP = {
    'In Production': 'Heat Press',
    'Picking': 'Picking',
    'Labeling': 'Labeling',
    'Film Matching': 'Film Matching',
    'QC Passed': 'Packing'
}

# Map PodFactory user_role to our role_configs.id
PODFACTORY_ROLE_TO_CONFIG_ID = {
    'Heat Pressing': 1,
    'Packing and Shipping': 2,
    'Picker': 3,
    'Labeler': 4,
    'Film Matching': 5
}

# Map PodFactory actions to role_configs.id
ACTION_TO_ROLE_ID = {
    'In Production': 1,      # Heat Pressing
    'QC Passed': 2,          # Packing and Shipping  
    'Picking': 3,            # Picker
    'Labeling': 4,           # Labeler
    'Film Matching': 5       # Film Matching
}


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PodFactorySync:
    def __init__(self):
        # Database configurations
        self.local_config = {
            'host': os.getenv('DB_HOST', 'db-mysql-sgp1-61022-do-user-16860331-0.h.db.ondigitalocean.com'),
            'port': int(os.getenv('DB_PORT', 25060)),
            'user': os.getenv('DB_USER', 'doadmin'),
            'password': os.getenv('DB_PASSWORD', 'AVNS_OWqdUdZ2Nw_YCkGI5Eu'),
            'database': 'productivity_tracker'
        }
        
        self.podfactory_config = {
            'host': 'db-mysql-sgp1-61022-do-user-16860331-0.h.db.ondigitalocean.com',
            'port': 25060,
            'user': 'doadmin',
            'password': 'AVNS_OWqdUdZ2Nw_YCkGI5Eu',
            'database': 'pod-report-stag',
            'ssl_disabled': False
        }
        
        # Your Flask API endpoint
        self.api_base = 'http://localhost:5000'
        
        # Timezone settings
        self.utc = pytz.UTC
        self.central = pytz.timezone('America/Chicago')
        
        # Add these for the smart sync
        self.ACTION_TO_DEPARTMENT_MAP = ACTION_TO_DEPARTMENT_MAP
        self.PODFACTORY_ROLE_TO_CONFIG_ID = PODFACTORY_ROLE_TO_CONFIG_ID
    
    def get_central_date(self):
        """Get current date in Central Time"""
        return datetime.now(self.central).date()
    
    def get_central_datetime(self):
        """Get current datetime in Central Time"""
        return datetime.now(self.central)
    
    def get_employee_mapping(self):
        """Get employee mappings using name-based matching"""
        conn = pymysql.connect(**self.local_config)
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        # Get all active employees
        cursor.execute("""
            SELECT 
                id as employee_id,
                name as display_name,
                LOWER(TRIM(name)) as normalized_name
            FROM employees
            WHERE is_active = 1
        """)
        
        employees = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Create comprehensive name mappings
        mappings = {}
        
        for emp in employees:
            employee_info = {
                'employee_id': emp['employee_id'],
                'name': emp['display_name']
            }
            
            # Store by normalized name
            normalized = emp['normalized_name']
            mappings[normalized] = employee_info
            
            # Also store common variations
            name_with_dot = normalized.replace(' ', '.')
            mappings[name_with_dot] = employee_info
            
            name_with_underscore = normalized.replace(' ', '_')
            mappings[name_with_underscore] = employee_info
            
            # Just first name
            first_name = normalized.split(' ')[0]
            if first_name not in mappings:
                mappings[first_name] = employee_info
            
            # Add last name only (for better matching)
            name_parts = normalized.split(' ')
            if len(name_parts) >= 2:
                last_name = name_parts[-1]
                if last_name not in mappings and len(last_name) > 3:
                    mappings[last_name] = employee_info
        
        # Manual mappings for special cases
        manual_mappings = {
            'vannesa apodaca': {'employee_id': 23, 'name': 'Vanessa Apodaca'},
            'hau nguyen 2': {'employee_id': 20, 'name': 'Hau Nguyen'},
        }
        
        mappings.update(manual_mappings)
        
        logger.info(f"Loaded {len(employees)} employees with {len(mappings)} name variations")
        return mappings 
    
    def extract_name_from_email(self, email):
        """Extract and normalize name from email address"""
        email_prefix = email.split('@')[0].lower()
        
        suffixes_to_remove = ['shp', 'ship', 'pack', 'pick', 'label', 'film', 'heatpress', 'hp']
        for suffix in suffixes_to_remove:
            if email_prefix.endswith(suffix):
                email_prefix = email_prefix[:-len(suffix)]
                break
        
        return email_prefix
    
    def find_employee_by_name(self, email, name_mappings, user_name=None):
        """Try to find employee using various name matching strategies"""
        if user_name:
            normalized_pf_name = user_name.lower().strip()
            
            if normalized_pf_name in name_mappings:
                return name_mappings[normalized_pf_name]
            
            name_with_dot = normalized_pf_name.replace(' ', '.')
            if name_with_dot in name_mappings:
                return name_mappings[name_with_dot]
            
            name_with_underscore = normalized_pf_name.replace(' ', '_')
            if name_with_underscore in name_mappings:
                return name_mappings[name_with_underscore]
            
            name_parts = normalized_pf_name.split()
            for part in name_parts:
                if len(part) > 3 and part in name_mappings:
                    logger.debug(f"Matched '{user_name}' to employee via name part '{part}'")
                    return name_mappings[part]
            
            significant_parts = [p for p in name_parts if len(p) > 2]
            if significant_parts:
                for mapped_name, employee_info in name_mappings.items():
                    if all(part in mapped_name for part in significant_parts):
                        logger.debug(f"Fuzzy matched '{user_name}' to '{employee_info['name']}'")
                        return employee_info
        
        # Fall back to email-based matching
        email_name = self.extract_name_from_email(email)
        
        if email_name in name_mappings:
            return name_mappings[email_name]
        
        normalized_name = email_name.replace('.', ' ').replace('_', ' ')
        if normalized_name in name_mappings:
            return name_mappings[normalized_name]
        
        dot_form = email_name.replace('_', '.')
        if dot_form in name_mappings:
            return name_mappings[dot_form]
        
        underscore_form = email_name.replace('.', '_')
        if underscore_form in name_mappings:
            return name_mappings[underscore_form]
        
        first_part = email_name.split('.')[0].split('_')[0]
        if first_part in name_mappings:
            return name_mappings[first_part]
        
        return None
    
    def get_last_sync_time(self):
        """Get the last successful sync time"""
        conn = pymysql.connect(**self.local_config)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS podfactory_sync_log (
                id INT AUTO_INCREMENT PRIMARY KEY,
                sync_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                records_synced INT DEFAULT 0,
                status VARCHAR(50),
                notes TEXT
            )
        """)
        
        cursor.execute("""
            SELECT MAX(sync_time) as last_sync 
            FROM podfactory_sync_log 
            WHERE status IN ('SUCCESS', 'PARTIAL')
        """)
        
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if result and result[0]:
            last_sync = result[0]
            if last_sync.tzinfo is None:
                # Assume database times are in UTC now
                last_sync = self.utc.localize(last_sync)
            return last_sync
        else:
            # If no previous sync, start from beginning of today Central Time
            today = self.get_central_date()
            start_of_day = self.central.localize(datetime.combine(today, datetime.min.time()))
            return start_of_day
    
    def log_sync(self, records_synced, status, notes=""):
        """Log sync operation"""
        conn = pymysql.connect(**self.local_config)
        cursor = conn.cursor()
        
        # Create table if it doesn't exist
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS podfactory_sync_log (
                id INT AUTO_INCREMENT PRIMARY KEY,
                sync_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                records_synced INT DEFAULT 0,
                status VARCHAR(50),
                notes TEXT
            )
        """)
        
        cursor.execute("""
            INSERT INTO podfactory_sync_log (sync_time, records_synced, status, notes)
            VALUES (UTC_TIMESTAMP(), %s, %s, %s)
        """, (records_synced, status, notes))
        
        conn.commit()
        cursor.close()
        conn.close()
        
    def check_existing_activities(self, activities):
        """Check which activities already exist to prevent duplicates"""
        if not activities:
            return set()
        
        conn = pymysql.connect(**self.local_config)
        cursor = conn.cursor()
        
        # Get all PodFactory IDs
        pf_ids = [str(a['id']) for a in activities]
        
        # Check which ones already exist
        if pf_ids:
            placeholders = ','.join(['%s'] * len(pf_ids))
            cursor.execute(f"""
                SELECT DISTINCT reference_id 
                FROM activity_logs 
                WHERE reference_id IN ({placeholders})
                AND source = 'podfactory'
            """, pf_ids)
            
            existing_ids = {row[0] for row in cursor.fetchall()}
        else:
            existing_ids = set()
        
        cursor.close()
        conn.close()
        
        return existing_ids
    
    def fetch_new_activities(self, since_time):
        """Fetch activities from PodFactory since last sync"""
        conn = mysql.connector.connect(**self.podfactory_config)
        cursor = conn.cursor(dictionary=True)
        
        # Convert since_time to UTC for PodFactory query
        if since_time.tzinfo:
            since_utc = since_time.astimezone(self.utc)
        else:
            since_central = self.central.localize(since_time)
            since_utc = since_central.astimezone(self.utc)
        
        end_utc = datetime.now(self.utc) + timedelta(hours=1)
        
        query = """
            SELECT 
                id,
                user_email,
                user_name,
                user_role,
                action,
                items_count,
                window_start,
                window_end,
                created_at
            FROM report_actions
            WHERE window_start > %s
            AND window_start <= %s
            AND items_count > 0
            ORDER BY window_start ASC
        """
        
        cursor.execute(query, (since_utc, end_utc))
        activities = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        logger.info(f"Fetched {len(activities)} activities since {since_time}")
        return activities
    
    def fetch_today_activities(self):
        """Fetch all activities for today in Central Time"""
        today = self.get_central_date()
        start_of_day = self.central.localize(datetime.combine(today, datetime.min.time()))
        
        logger.info(f"Fetching all activities for {today} (Central Time)")
        return self.fetch_new_activities(start_of_day)
    
    def convert_to_central(self, utc_dt):
        """Convert UTC datetime to Central Time for display only"""
        if utc_dt:
            if utc_dt.tzinfo is None:
                utc_dt = self.utc.localize(utc_dt)
            return utc_dt.astimezone(self.central)
        return None
    
    def send_to_api(self, activity_data):
        """Send activity to Flask API"""
        try:
            response = requests.post(
                f"{self.api_base}/api/dashboard/activities/activity",
                json=activity_data,
                headers={
                    'Content-Type': 'application/json',
                    'X-API-Key': 'dev-api-key-123'
                },
                timeout=5
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('status') == 'duplicate':
                    logger.debug(f"Activity already exists: {activity_data['metadata']['podfactory_id']}")
                else:
                    logger.debug(f"Activity created successfully: {result.get('activity_id')}")
                return True
            else:
                logger.error(f"API error: {response.status_code} - {response.text}")
                return False
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to send to API: {e}")
            return False
        
    def send_activities_batch(self, activities_batch):
        """Send multiple activities to Flask API in one request"""
        if not activities_batch:
            return True
            
        try:
            bulk_endpoint = f"{self.api_base}/api/dashboard/activities/bulk"
            headers = {
                'Content-Type': 'application/json',
                'X-API-Key': 'dev-api-key-123'
            }
            
            response = requests.post(
                bulk_endpoint,
                json=activities_batch,
                headers=headers,
                timeout=120
            )
            
            if response.status_code == 200:
                logger.info(f"✅ Successfully sent batch of {len(activities_batch)} activities")
                return True
            else:
                logger.error(f"❌ Bulk API error: {response.status_code} - {response.text}")
                success_count = 0
                for activity in activities_batch:
                    if self.send_to_api(activity):
                        success_count += 1
                logger.info(f"Sent {success_count}/{len(activities_batch)} activities individually")
                return success_count > 0
                
        except Exception as e:
            logger.error(f"Failed to send batch: {e}")
            return False

    def sync_activities(self, use_last_sync=True):
        """Main sync function - now stores times in UTC"""
        logger.info("="*60)
        logger.info("Starting PodFactory sync (UTC storage mode)...")
        logger.info(f"Current Central Time: {self.get_central_datetime()}")
        
        name_mappings = self.get_employee_mapping()
        if not name_mappings:
            logger.error("No employee mappings found!")
            return 0, 0
        
        if use_last_sync:
            last_sync = self.get_last_sync_time()
            logger.info(f"Last sync was at: {last_sync}")
            activities = self.fetch_new_activities(last_sync)
        else:
            activities = self.fetch_today_activities()
        
        if not activities:
            logger.info("No new activities to sync")
            self.log_sync(0, "SUCCESS", "No new activities")
            return 0, 0
        
        # CHECK DATABASE FOR EXISTING ACTIVITIES (REPLACE OLD DEDUPLICATION)
        existing_ids = self.check_existing_activities(activities)
        if existing_ids:
            original_count = len(activities)
            activities = [a for a in activities if str(a['id']) not in existing_ids]
            skipped_existing = original_count - len(activities)
            logger.info(f"Skipping {skipped_existing} activities that already exist in database")
            
            if not activities:
                logger.info("All activities already synced")
                self.log_sync(0, "SUCCESS", f"All {original_count} activities already in database")
                return 0, 0
        # END OF DATABASE CHECK
        
        # Group activities by date for logging (convert to CT for display)
        activities_by_date = {}
        for activity in activities:
            window_central = self.convert_to_central(activity['window_start'])
            date_key = window_central.date()
            if date_key not in activities_by_date:
                activities_by_date[date_key] = 0
            activities_by_date[date_key] += 1
        
        logger.info("Activities by date (Central Time):")
        for date_key, count in sorted(activities_by_date.items()):
            logger.info(f"  {date_key}: {count} activities")
        
        # Process activities
        success_count = 0
        error_count = 0
        skipped_count = 0
        skipped_emails = {}
        
        batch_size = 50
        activities_batch = []
        employee_names_batch = []
        
        for activity in activities:
            employee = self.find_employee_by_name(
                activity['user_email'], 
                name_mappings, 
                user_name=activity.get('user_name')
            )
            
            if not employee:
                email = activity['user_email']
                if email not in skipped_emails:
                    skipped_emails[email] = 0
                skipped_emails[email] += 1
                skipped_count += 1
                continue
            
            # Keep times in UTC
            window_start = activity['window_start']
            window_end = activity['window_end']
            
            # Ensure timezone awareness
            if window_start and window_start.tzinfo is None:
                window_start = self.utc.localize(window_start)
            if window_end and window_end.tzinfo is None:
                window_end = self.utc.localize(window_end)
            
            duration = 0
            if window_start and window_end:
                duration = int((window_end - window_start).total_seconds() / 60)
            
            user_role = activity.get('user_role', '')
            department = ACTION_TO_DEPARTMENT_MAP.get(activity['action'], 'Unknown')
            scan_type = 'batch_scan' if role_id in [3, 4, 5] else 'item_scan'
            
            role_id = ACTION_TO_ROLE_ID.get(activity['action'], 3)
            if activity['action'] == 'QC Passed' and role_id == 3:
                logger.warning(f"WARNING: QC Passed getting role_id 3! Action value: '{activity['action']}'")
            user_role = activity.get('user_role', '')
            # Convert to Central ONLY for logging display
            window_start_central = self.convert_to_central(window_start)
            
            # Prepare data for API - times in UTC
            api_data = {
                'employee_id': employee['employee_id'],
                'scan_type': scan_type,
                'quantity': activity['items_count'] or 1,
                'department': department,
                'timestamp': window_start.isoformat(),  # UTC
                'window_end': window_end.isoformat() if window_end else None,  # UTC
                'metadata': {
                    'source': 'podfactory',
                    'podfactory_id': str(activity['id']),
                    'action': activity['action'],
                    'user_role': user_role,
                    'duration_minutes': duration,
                    'role_id': role_id,
                    'timezone': 'UTC'  # Mark as UTC
                }
            }
            
            # Log if activity ID is missing
            if not activity.get('id'):
                logger.warning(f"Activity missing ID: {activity.get('user_email')} - {activity.get('action')} - {activity.get('items_count')} items at {window_start}")
            
            activities_batch.append(api_data)
            employee_names_batch.append({
                'name': employee['name'],
                'action': activity['action'],
                'items': activity['items_count'],
                'time': window_start_central.strftime('%I:%M %p CT')  # Display in CT
            })
            
            # Send batch when it reaches the size limit
            if len(activities_batch) >= batch_size:
                if self.send_activities_batch(activities_batch):
                    success_count += len(activities_batch)
                    for emp in employee_names_batch:
                        logger.info(f"✅ Synced: {emp['name']} - {emp['action']} ({emp['items']} items) - {emp['time']}")
                else:
                    error_count += len(activities_batch)
                    for emp in employee_names_batch:
                        logger.error(f"❌ Failed: {emp['name']} - {emp['action']}")
                
                activities_batch = []
                employee_names_batch = []
                time.sleep(0.5)
        
        # Send remaining activities
        if activities_batch:
            if self.send_activities_batch(activities_batch):
                success_count += len(activities_batch)
                for emp in employee_names_batch:
                    logger.info(f"✅ Synced: {emp['name']} - {emp['action']} ({emp['items']} items) - {emp['time']}")
            else:
                error_count += len(activities_batch)
                for emp in employee_names_batch:
                    logger.error(f"❌ Failed: {emp['name']} - {emp['action']}")
        
        if skipped_emails:
            logger.warning("\nSkipped emails (no name mapping found):")
            for email, count in sorted(skipped_emails.items()):
                extracted_name = self.extract_name_from_email(email)
                logger.warning(f"  {email} ({count} activities) - extracted: '{extracted_name}'")
        
        status = "SUCCESS" if error_count == 0 else "PARTIAL"
        notes = f"Synced {success_count} activities, {error_count} errors, {skipped_count} skipped"
        self.log_sync(success_count, status, notes)
        
        logger.info("="*60)
        logger.info(f"Sync complete! ✅ Success: {success_count}, ❌ Errors: {error_count}, ⏭️ Skipped: {skipped_count}")
        logger.info("="*60)
        if success_count > 0:
            self.trigger_score_update()
        return success_count, error_count
        
    def trigger_score_update(self):
        """Trigger ProductivityCalculator to update daily_scores"""
        try:
            # Option 1: Direct database update
            conn = pymysql.connect(**self.local_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO daily_scores (employee_id, score_date, items_processed, points_earned, active_minutes, clocked_minutes, efficiency_rate)
                SELECT 
                    al.employee_id,
                    CURDATE(),
                    SUM(al.items_count),
                    SUM(al.items_count * rc.multiplier),
                    0, 0, 0
                FROM activity_logs al
                JOIN role_configs rc ON rc.id = al.role_id
                WHERE DATE(al.window_start) = CURDATE()
                    AND al.source = 'podfactory'
                GROUP BY al.employee_id
                ON DUPLICATE KEY UPDATE
                    items_processed = VALUES(items_processed),
                    points_earned = VALUES(points_earned),
                    updated_at = NOW()
            """)
            
            conn.commit()
            cursor.close()
            conn.close()
            
            logger.info("Triggered daily scores update after sync")
            
        except Exception as e:
            logger.error(f"Failed to update scores: {e}")
    
    def sync_date_range(self, start_date, end_date):
        """Sync activities for a specific date range (dates in Central Time)"""
        logger.info(f"Syncing activities from {start_date} to {end_date}")
        
        # Convert CT dates to UTC boundaries
        start_ct = self.central.localize(datetime.combine(start_date, datetime.min.time()))
        end_ct = self.central.localize(datetime.combine(end_date, datetime.max.time()))
        start_utc = start_ct.astimezone(self.utc)
        end_utc = end_ct.astimezone(self.utc)
        
        logger.info(f"UTC range: {start_utc} to {end_utc}")
        
        conn = mysql.connector.connect(**self.podfactory_config)
        cursor = conn.cursor(dictionary=True)
        
        query = """
            SELECT 
                id,
                user_email,
                user_name,
                user_role,
                action,
                items_count,
                window_start,
                window_end,
                created_at
            FROM report_actions
            WHERE window_start >= %s
            AND window_start <= %s
            AND items_count > 0
            ORDER BY window_start ASC
        """
        
        cursor.execute(query, (start_utc, end_utc))
        activities = cursor.fetchall()
        cursor.close()
        conn.close()
        
        logger.info(f"Found {len(activities)} activities for {start_date} to {end_date}")
        
        if not activities:
            logger.info("No activities to sync for this date range")
            self.log_sync(0, "SUCCESS", f"No activities for {start_date} to {end_date}")
            return 0, 0
        
        original_fetch = self.fetch_today_activities
        self.fetch_today_activities = lambda: activities
        
        try:
            success, errors = self.sync_activities(use_last_sync=False)
        finally:
            self.fetch_today_activities = original_fetch
        
        return success, errors
    
    def test_connection(self):
        """Test connections to both databases and API"""
        print("\nTesting connections...")
        print("-"*40)
        
        try:
            conn = pymysql.connect(**self.local_config)
            conn.close()
            print("✅ Local database connection OK")
        except Exception as e:
            print(f"❌ Local database error: {e}")
            return False
        
        try:
            conn = mysql.connector.connect(**self.podfactory_config)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM report_actions WHERE created_at >= CURDATE()")
            count = cursor.fetchone()[0]
            cursor.close()
            conn.close()
            print(f"✅ PodFactory database connection OK ({count} activities today)")
        except Exception as e:
            print(f"❌ PodFactory database error: {e}")
            return False
        
        try:
            response = requests.get(f"{self.api_base}/health", timeout=5)
            if response.status_code == 200:
                print("✅ Flask API connection OK")
            else:
                print(f"⚠️ Flask API returned status {response.status_code}")
        except Exception as e:
            print(f"⚠️ Flask API not responding (will test during sync)")
        
        print("-"*40)
        return True

# Main execution
if __name__ == "__main__":
    sync = PodFactorySync()
    
    if not sync.test_connection():
        print("\n❌ Fix connection issues before running sync")
        exit(1)
    
    print("\n" + "="*50)
    print("PodFactory Activity Sync (UTC Storage Mode)")
    print(f"Current Central Time: {sync.get_central_datetime().strftime('%Y-%m-%d %I:%M:%S %p')}")
    print("="*50)
    print("\nChoose sync mode:")
    print("1. Sync new activities (since last sync)")
    print("2. Sync all of today's activities") 
    print("3. Run continuous sync (every 5 minutes)")
    print("4. Sync specific date range")
    print("5. Sync last 7 days")
    print("="*50)
    
    choice = input("\nEnter choice (1-5): ")
    
    if choice == "1":
        sync.sync_activities(use_last_sync=True)
    
    elif choice == "2":
        sync.sync_activities(use_last_sync=False)
    
    elif choice == "3":
        interval = input("Enter sync interval in minutes (default 5): ")
        interval = int(interval) if interval else 5
        sync.run_continuous_sync(interval_minutes=interval)
    
    elif choice == "4":
        print("\nEnter date range (YYYY-MM-DD format)")
        start_str = input("Start date: ")
        end_str = input("End date: ")
        try:
            start_date = datetime.strptime(start_str, '%Y-%m-%d').date()
            end_date = datetime.strptime(end_str, '%Y-%m-%d').date()
            sync.sync_date_range(start_date, end_date)
        except ValueError:
            print("Invalid date format!")
    
    elif choice == "5":
        # Sync last 7 days
        today = sync.get_central_date()
        for days_ago in range(7, 0, -1):
            sync_date = today - timedelta(days=days_ago)
            print(f"\nSyncing {sync_date}...")
            sync.sync_date_range(sync_date, sync_date)
    
    else:
        print("Invalid choice!")
        
    print("\n✨ Done!")